# 02  Preparing for Test Automation – 180 minutes

## Keywords:
API testing, GUI testing, testability

## Learning Objectives for Chapter 2:
### 2.1 Understand the Configuration of an Infrastructure to Enable Test Automation
- TAE-2.1.1 (K2) Describe the configuration needs of an infrastructure that enable implementation of test
automation
- TAE-2.1.2 (K2) Explain how test automation is leveraged within different environments
### 2.2 Evaluation Process for Selecting the Right Tools and Strategies
- TAE-2.2.1 (K4) Analyze a system under test to determine the appropriate test automation solution
- TAE-2.2.2 (K4) Illustrate the technical findings of a tool evaluation

## 2.1 Understand the Configuration of an Infrastructure to Enable Test Automation
### 2.1.1 Describe the Configuration Needs of an Infrastructure that Enable Implementation of Test Automation

The testability of the SUT should be **designed and implemented in parallel with other SUT features.** This is typically done by a software architect, as testability is a non-functional requirement, often with input from a TAE to pinpoint areas for improvement.


To improve the testability of the SUT, various solutions can be used with different configuration needs, such as:

- **Accessibility identifiers:** Automatically generated by development frameworks or manually set by developers.
- **System environment variables:** Application parameters can be adjusted for easier testing through administration.
- **Deployment variables:** Similar to system variables but set before deployment begins.

**Designing** for testability of a SUT consists of the following aspects:

- **Observability:** The SUT must offer interfaces that provide insight, allowing test cases to check if actual results match expected results.
- **Controllability:** The SUT should offer interfaces for performing actions, such as UI elements, function calls, or communication protocols (e.g., TCP/IP, USB), and signals for physical or logical switches on environment variables.
- **Architecture transparency:** The architecture documentation must clearly define components and interfaces, ensuring observability, controllability, and quality across all test levels.

### 2.1.2 Explain How Test Automation is Leveraged within Different Environments

Different types of automated tests can be executed in different environments. These environments can
differ between projects and methodologies, and most projects have one or more environments to utilize for
testing. From a technical point of view these environments can be created from containers, virtualization
software, and using other approaches.

### A set of possible environments:

- **Local development environment:**
This is where software is first created, and components are tested with automation to verify functionality. Test types include component, GUI, and API testing. Using an IDE, white-box testing can detect coding issues early.

- **Build environment:** Its main role is to build software and run tests to verify the build's correctness in a DevOps setup. It can be local or a CI/CD agent, where component tests and static analysis are done without deployment to other environments.

- **Integration environment:** After low-level testing, the system is tested in a fully integrated environment. Automated UI or API tests are run here, focusing on black-box testing (system integration/acceptance). Monitoring begins in this environment to detect defects during SUT use.

- **Preproduction environment:** This environment closely mirrors production and is used for assessing non-functional qualities (e.g., performance). User acceptance testing is often conducted here by stakeholders, and the automated test suite can be executed. Monitoring is also present.

- **Production/Operational environment:** Used to evaluate functional and non-functional qualities in real-time, with users interacting with the deployed system. Testing practices like canary releases, blue/green deployments, and A/B testing are utilized alongside monitoring.

## 2.2 Evaluation Process for Selecting the Right Tools and Strategies
### 2.2.1 Analyze a System Under Test to Determine the Appropriate Test Automation Solution

Each SUT can differ, but several factors and **characteristics can be analyzed to create a successful test automation solution (TAS).** When investigating an SUT, TAEs must gather requirements based on its scope and capabilities. Different applications (e.g., web services, mobile, web) require different technical approaches to test automation. This **investigation is best done collaboratively with stakeholders** (e.g., manual testers, business analysts) to identify risks and mitigation strategies for a future-proof TAS.

### Key requirements for a test automation approach and architecture:

- Which test process activities to automate (e.g., test management, design, generation, execution)
- Supported test levels
- Supported test types
- Required test roles and skill sets
- Supported software products, lines, and families (span and lifetime of the TAS)
- SUT compatibility with the TAS
- Availability and quality of test data
- Methods to emulate unreachable cases (e.g., 3rd party applications)

### 2.2.2 Illustrate the Technical Findings of a Tool Evaluation
Once the SUT is analyzed and requirements are gathered from stakeholders, **suitable test automation tools can be considered.** It’s possible that **no single tool will meet all requirements**, and stakeholders should be aware of this.

A **comparison table** is useful for documenting findings and reflecting on direct and indirect requirements. This table helps stakeholders see differences between tools based on specific needs. **Tools are listed in columns, requirements in rows,** and the cells provide details on each tool's properties and priorities concerning each requirement.

**Test automation tools should be evaluated** to ensure they meet the identified requirements.

### Key considerations for comparing tools:

- **The language/technology** of the tool and compatibility with IDE tools
- **Configurability**, support for different test environments, run configurations, and use of dynamic/static setup values
- **Ability to manage test data**, potentially integrated with a central repository for version control
- **Suitability** for different test types, as different tools may be needed
- **Reporting capabilities** to align with the project's test reporting needs
- **Integration with other project tools** (e.g., CI/CD, task tracking, test management, reporting)
- **Ability to expand the test architecture**, and assess scalability, maintainability, modifiability, compatibility, and reliability

The comparison table helps propose the best tool or tool set for SUT test automation. Once a proposal is made, it should be presented to stakeholders for approval.

### Example of comparison table*:

| **Criteria**                                  | **Selenium**                             | **Cypress**                             | **TestComplete**                         | **Katalon Studio**                        |
|-----------------------------------------------|------------------------------------------|------------------------------------------|-------------------------------------------|-------------------------------------------|
| **Language/Technology Compatibility**         | Supports multiple languages (Java, C#, Python, etc.) | JavaScript (Node.js)                    | Supports multiple languages (JavaScript, VBScript, Python) | Supports multiple languages (Java, Groovy) |
| **IDE Integration**                           | Integrates with IntelliJ, Eclipse, and VS Code | Integrates with VS Code, IDE built-in   | TestComplete IDE, Integrates with IntelliJ | Integrated IDE, Supports IntelliJ, Eclipse |
| **Configurability**                           | High configurability for test setups and environments | Configurable, but less flexible than Selenium | Extensive support for test configurations | Configurable with a good range of options |
| **Support for Dynamic/Static Setup Values**    | Supports both dynamic and static setup values | Supports dynamic values well            | Supports both, with robust configuration tools | Supports both, good for static and dynamic data |
| **Test Data Management**                      | Needs external integration for test data management | Limited support for test data management | Built-in test data management and version control | Integrated test data management with version control |
| **Test Types Supported**                      | Functional, regression, performance, UI, and more | Primarily end-to-end functional and UI tests | Supports functional, regression, load, and API testing | Supports functional, regression, load, and API testing |
| **Reporting Capabilities**                    | Needs third-party integration for reporting (e.g., Allure) | Built-in reporting with customizable dashboards | Robust built-in reporting with detailed logs | Built-in customizable reports with analytics |
| **Integration with CI/CD & Project Tools**    | Integrates with Jenkins, Git, Jira, TestRail | Native integration with Jenkins, GitLab, CircleCI | Integrates with Jenkins, Azure DevOps, Jira, etc. | Integrates with Jenkins, Git, Jira, and more |
| **Scalability & Maintainability**             | Highly scalable but can require more maintenance | Fast and easy to maintain but less scalable for large projects | Scalable with strong maintainability features | Moderate scalability with good maintenance support |
| **Modifiability**                             | High, but requires programming expertise | Easy to modify tests but limited to JavaScript | Good modifiability, supports both scripting and record-playback | Moderately modifiable, scripting support available |
| **Compatibility with Different Test Environments** | Supports web, mobile, and desktop applications | Best for web applications (Chrome, Firefox, Edge) | Supports web, desktop, and mobile apps | Supports web, mobile, desktop, and API tests |
| **Reliability**                               | High, but requires more setup and maintenance | Reliable, fast execution with minimal flakiness | Highly reliable with built-in features to reduce test flakiness | Reliable, though execution speed can vary |
| **Cost**                                      | Free, open-source                        | Free, open-source                        | Paid license required, with a free trial   | Freemium model with paid enterprise features |

#### Analysis

1. **Selenium** is highly flexible and customizable, making it ideal for larger teams with diverse skill sets. However, it requires more setup and ongoing maintenance. Best suited for projects requiring diverse test types and languages.

2. **Cypress** is a fast and easy-to-use tool, best suited for end-to-end web application testing. It’s a great choice for smaller projects or when ease of use and speed of execution are prioritized.

3. **TestComplete** is a paid tool with extensive functionality, especially for teams that need robust support for desktop, web, and mobile apps. Its reporting and test data management are more advanced but come with higher costs.

4. **Katalon Studio** provides a good balance between configurability and ease of use. It’s ideal for teams looking for a cost-effective solution with decent functionality for API, mobile, and web testing.

#### Conclusion
- **Best for large, complex projects:** **Selenium** (for its flexibility and wide range of integrations)
- **Best for fast, easy web testing:** **Cypress**
- **Best for enterprise-level testing with desktop, mobile, and web:** **TestComplete**
- **Best for teams needing ease of use and a balance of features:** **Katalon Studio**
